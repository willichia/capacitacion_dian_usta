{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d255c28-5ee0-4267-a41f-8c036039c114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Taler de Arquitecturas de Datos\n",
    "**üî¨ Paso 1: Creando Nuestro Universo de Datos**\n",
    "\n",
    "¬°Bienvenidos al taller pr√°ctico! Antes de poder construir y comparar arquitecturas, necesitamos la materia prima: los datos.\n",
    "\n",
    "En esta celda, ejecutaremos un script de PySpark que simula un ecosistema de datos completo para una empresa de E-commerce. Este no es un dataset est√°tico; es un generador que crear√° un universo de datos controlado y realista para nuestro laboratorio.\n",
    "\n",
    "**¬øQu√© Genera este Script?**\n",
    "Este c√≥digo crear√° varios \"activos de datos\" que simulan las diferentes fuentes que encontrar√≠as en un entorno real:\n",
    "\n",
    "* **Tablas Estructuradas:** usuarios, productos y pedidos, simulando los datos de una base de datos transaccional.\n",
    "* **Logs Semi-Estructurados:** logs_web con la actividad de navegaci√≥n de los usuarios.\n",
    "* **Archivos No Estructurados:** Facturas en formato de texto guardadas en DBFS, simulando la ingesta de archivos como PDFs.\n",
    "* **Documentos Complejos:** Un perfil_360 de cliente, simulando c√≥mo se ver√≠an los datos en una base de datos NoSQL como Cosmos DB o MongoDB.\n",
    "\n",
    "**Puntos Clave:**\n",
    "* **Librer√≠a Faker:** Usamos esta librer√≠a para generar datos que parecen reales (nombres, emails, fechas, etc.).\n",
    "* **Semilla de Reproducibilidad:** Hemos fijado una SEMILLA para que cada vez que se ejecute el script, genere exactamente los mismos datos. Elemental para que todos obtengamos los mismos resultados en los ejercicios.\n",
    "\n",
    "**Acci√≥n:** Ejecuta esta celda para generar todos los DataFrames y archivos necesarios. ¬°Este es el punto de partida para nuestro viaje a trav√©s de las arquitecturas de datos!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3a4adb-d601-4d98-91d5-68dd1894eaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script para generar datos simulados para el Taller de Arquitecturas de Datos\n",
    "# Este script utiliza PySpark y la librer√≠a Faker para crear datos realistas,\n",
    "# incluyendo la simulaci√≥n de archivos de texto (facturas) y logs web.\n",
    "#\n",
    "# Instrucciones en Databricks:\n",
    "# 1. Aseg√∫rate de que la librer√≠a 'Faker' est√© instalada en tu cluster.\n",
    "#    Puedes hacerlo a trav√©s de la UI del cluster en la pesta√±a \"Libraries\".\n",
    "#    - PyPI -> package: Faker\n",
    "# 2. Copia y pega este c√≥digo en una celda de un notebook de Databricks.\n",
    "# 3. Ejecuta la celda.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, ArrayType, MapType\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Inicializar Faker para generar datos falsos\n",
    "fake = Faker('es_ES') # Usar localizaci√≥n en espa√±ol para datos m√°s realistas\n",
    "\n",
    "# --- SEMILLA PARA REPRODUCIBILIDAD ---\n",
    "# Establecemos una semilla para que los datos generados sean siempre los mismos en cada ejecuci√≥n.\n",
    "# Esto es crucial para que todos los estudiantes trabajen con el mismo dataset.\n",
    "SEED = 42\n",
    "Faker.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- 1. Inicializaci√≥n de Spark Session ---\n",
    "# En un notebook de Databricks, la sesi√≥n de Spark ya est√° creada como 'spark'.\n",
    "print(\"Spark Session iniciada.\")\n",
    "\n",
    "# --- 2. Funciones de Generaci√≥n de Datos Relacionales ---\n",
    "\n",
    "def generar_usuarios(n=100):\n",
    "    \"\"\"Genera una lista de diccionarios de usuarios.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_usuario': 1000 + i,\n",
    "            'nombre': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'fecha_registro': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'ciudad': fake.city()\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_productos(n=50):\n",
    "    \"\"\"Genera una lista de diccionarios de productos.\"\"\"\n",
    "    categorias = ['Electr√≥nica', 'Hogar', 'Ropa', 'Libros', 'Deportes']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_producto': 2000 + i,\n",
    "            'nombre_producto': fake.word().capitalize() + \" \" + fake.word(),\n",
    "            'categoria': random.choice(categorias),\n",
    "            'precio_unitario': round(random.uniform(5.0, 250.0), 2)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_pedidos(usuarios, productos, n=500):\n",
    "    \"\"\"Genera una lista de diccionarios de pedidos, vinculando usuarios y productos.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        producto = random.choice(productos)\n",
    "        cantidad = random.randint(1, 5)\n",
    "        data.append({\n",
    "            'id_pedido': 3000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'id_producto': producto['id_producto'],\n",
    "            'cantidad': cantidad,\n",
    "            'monto': round(cantidad * producto['precio_unitario'], 2),\n",
    "            'fecha_pedido': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_logs_web(usuarios, n=2000):\n",
    "    \"\"\"Genera una lista de diccionarios de logs de visitas web m√°s realistas.\"\"\"\n",
    "    paginas = ['/inicio', '/producto/detalle', '/carrito', '/checkout', '/perfil']\n",
    "    metodos = ['GET', 'GET', 'GET', 'POST', 'GET']\n",
    "    status = [200, 200, 200, 200, 404, 500]\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        data.append({\n",
    "            'id_log': 4000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'pagina_visitada': random.choice(paginas),\n",
    "            'metodo_http': random.choice(metodos),\n",
    "            'codigo_estado': random.choice(status),\n",
    "            'timestamp': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "print(\"Funciones de generaci√≥n de datos creadas.\")\n",
    "\n",
    "# --- 3. Creaci√≥n de DataFrames de Spark ---\n",
    "\n",
    "usuarios_data = generar_usuarios(100)\n",
    "productos_data = generar_productos(50)\n",
    "pedidos_data = generar_pedidos(usuarios_data, productos_data, 500)\n",
    "logs_web_data = generar_logs_web(usuarios_data, 2000)\n",
    "\n",
    "usuarios_df = spark.createDataFrame(usuarios_data)\n",
    "productos_df = spark.createDataFrame(productos_data)\n",
    "pedidos_df = spark.createDataFrame(pedidos_data)\n",
    "logs_web_df = spark.createDataFrame(logs_web_data)\n",
    "\n",
    "print(\"\\n--- DataFrames Relacionales Creados ---\")\n",
    "usuarios_df.show(3)\n",
    "productos_df.show(3)\n",
    "pedidos_df.show(3)\n",
    "logs_web_df.show(3, truncate=False)\n",
    "\n",
    "# --- 4. Generaci√≥n de Datos No Estructurados (Simulaci√≥n de Archivos) ---\n",
    "\n",
    "def generar_y_guardar_facturas_texto(pedidos, usuarios, productos, ruta_dbfs):\n",
    "    \"\"\"\n",
    "    Simula la creaci√≥n de archivos de facturas (como si fueran PDFs convertidos a texto).\n",
    "    Guarda cada factura como un archivo .txt en la ruta de DBFS especificada.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerando archivos de facturas en la ruta: {ruta_dbfs}\")\n",
    "    \n",
    "    # Crear un mapa de usuarios y productos para b√∫squeda f√°cil\n",
    "    mapa_usuarios = {u['id_usuario']: u for u in usuarios}\n",
    "    mapa_productos = {p['id_producto']: p for p in productos}\n",
    "    \n",
    "    # Asegurarse de que el directorio existe\n",
    "    dbutils.fs.mkdirs(ruta_dbfs)\n",
    "    \n",
    "    # Tomar una muestra de 50 pedidos para generar facturas\n",
    "    for pedido in random.sample(pedidos, 50):\n",
    "        usuario = mapa_usuarios.get(pedido['id_usuario'])\n",
    "        producto = mapa_productos.get(pedido['id_producto'])\n",
    "        \n",
    "        if not usuario or not producto:\n",
    "            continue\n",
    "            \n",
    "        # Crear el contenido de la factura como un string\n",
    "        contenido_factura = f\"\"\"\n",
    "        ========================================\n",
    "        FACTURA ELECTR√ìNICA\n",
    "        ========================================\n",
    "        \n",
    "        N√∫mero de Factura: INV-{pedido['id_pedido']}\n",
    "        Fecha: {pedido['fecha_pedido'].strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \n",
    "        --- Cliente ---\n",
    "        ID Cliente: {usuario['id_usuario']}\n",
    "        Nombre: {usuario['nombre']}\n",
    "        Email: {usuario['email']}\n",
    "        Ciudad: {usuario['ciudad']}\n",
    "        \n",
    "        --- Detalles del Pedido ---\n",
    "        ID Pedido: {pedido['id_pedido']}\n",
    "        \n",
    "        Descripci√≥n                 Cantidad      Precio Unit.      Total\n",
    "        -----------------------------------------------------------------\n",
    "        {producto['nombre_producto']:<28}{pedido['cantidad']:<14}${producto['precio_unitario']:<16.2f}${pedido['monto']:.2f}\n",
    "        \n",
    "        ========================================\n",
    "        TOTAL A PAGAR: ${pedido['monto']:.2f}\n",
    "        ========================================\n",
    "        \"\"\"\n",
    "        \n",
    "        # Guardar el string en un archivo en DBFS\n",
    "        nombre_archivo = f\"factura_{pedido['id_pedido']}.txt\"\n",
    "        dbutils.fs.put(f\"{ruta_dbfs}/{nombre_archivo}\", contenido_factura, overwrite=True)\n",
    "        \n",
    "    print(f\"Se generaron 50 archivos de factura de ejemplo en {ruta_dbfs}\")\n",
    "    print(\"Los estudiantes pueden usar Auto Loader o spark.read.text() para ingerir estos datos.\")\n",
    "\n",
    "# Ejecutar la funci√≥n para generar los archivos de factura\n",
    "ruta_facturas = \"/tmp/facturas_raw\"\n",
    "generar_y_guardar_facturas_texto(pedidos_data, usuarios_data, productos_data, ruta_facturas)\n",
    "\n",
    "\n",
    "# --- 5. Funci√≥n para Generar Documentos de Perfil 360 (Simulaci√≥n NoSQL) ---\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def crear_perfil_360(usuarios_df, pedidos_df, logs_web_df):\n",
    "    \"\"\"\n",
    "    Combina los DataFrames para crear un perfil 360 de cada cliente (simulaci√≥n de Cosmos DB/MongoDB).\n",
    "    \"\"\"\n",
    "    pedidos_agrupados = pedidos_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"id_pedido\", \"id_producto\", \"monto\", \"fecha_pedido\")).alias(\"pedidos\"))\n",
    "    logs_agrupados = logs_web_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"pagina_visitada\", \"timestamp\")).alias(\"actividad_web\"))\n",
    "    \n",
    "    perfil_360_df = usuarios_df.join(pedidos_agrupados, \"id_usuario\", \"left\").join(logs_agrupados, \"id_usuario\", \"left\")\n",
    "    return perfil_360_df\n",
    "\n",
    "print(\"\\n--- Generando Documentos de Perfil 360 (Simulaci√≥n NoSQL) ---\")\n",
    "perfil_360_df = crear_perfil_360(usuarios_df, pedidos_df, logs_web_df)\n",
    "\n",
    "print(\"Mostrando una muestra de los perfiles 360:\")\n",
    "perfil_360_df.show(3, truncate=False)\n",
    "\n",
    "# --- 6. Visualizaci√≥n de un Documento JSON ---\n",
    "\n",
    "print(\"\\n--- Ejemplo de un Documento JSON para el Perfil 360 ---\")\n",
    "primer_perfil_json = perfil_360_df.first().asDict(recursive=True)\n",
    "print(json.dumps(primer_perfil_json, indent=4, default=str))\n",
    "\n",
    "#\n",
    "# Fin del Script\n",
    "# Los DataFrames y los archivos de texto ya est√°n listos para el taller.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd77b72a-089b-4b4b-b5f6-0582d20764c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üèóÔ∏è Paso 2: Construyendo la Capa de Bronce (Bronze)\n",
    "Ya hemos generado nuestros DataFrames en memoria. Ahora, vamos a dar el primer paso para construir nuestro Lakehouse: persistir los datos crudos.\n",
    "\n",
    "En la metodolog√≠a de Databricks, la primera capa se conoce como Bronce. Esta capa contiene los datos en su estado m√°s puro, tal como llegan de los sistemas de origen. Es nuestra copia de seguridad y el punto de partida para cualquier pipeline de datos.\n",
    "\n",
    "Acci√≥n\n",
    "En la siguiente celda, vamos a guardar cada uno de nuestros DataFrames relacionales (usuarios_df, productos_df, etc.) como una tabla Delta en el cat√°logo de Databricks. Usaremos el sufijo _bronze para identificar claramente que pertenecen a esta capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0e4434-2075-4c42-94ef-f6f5f3e38151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Celda 2: Guardar DataFrames Relacionales como Tablas Delta (Capa Bronce)\n",
    "#\n",
    "# Objetivo: Persistir los datos crudos que generamos en el paso anterior\n",
    "# en el Unity Catalog (o Hive Metastore) de Databricks.\n",
    "# Usamos el formato Delta Lake por sus ventajas (ACID, Time Travel, etc.).\n",
    "#\n",
    "\n",
    "# --- Definir el nombre de la base de datos (o schema) ---\n",
    "# Es una buena pr√°ctica organizar las tablas en un schema.\n",
    "# Aseg√∫rate de que este schema exista o de tener permisos para crearlo.\n",
    "# Si no usas Unity Catalog, puedes omitir el cat√°logo 'main'.\n",
    "db_name = \"curso_arquitecturas\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "print(f\"Usando la base de datos: {db_name}\")\n",
    "\n",
    "# --- Guardar cada DataFrame como una tabla Delta ---\n",
    "\n",
    "# Guardar la tabla de usuarios\n",
    "usuarios_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"usuarios_bronze\")\n",
    "\n",
    "print(\"Tabla 'usuarios_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de productos\n",
    "productos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"productos_bronze\")\n",
    "\n",
    "print(\"Tabla 'productos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de pedidos\n",
    "pedidos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"pedidos_bronze\")\n",
    "\n",
    "print(\"Tabla 'pedidos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de logs web\n",
    "logs_web_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"logs_web_bronze\")\n",
    "\n",
    "print(\"Tabla 'logs_web_bronze' guardada exitosamente.\")\n",
    "\n",
    "print(\"\\n¬°Proceso completado! Las 4 tablas de la capa Bronce ya est√°n disponibles en el cat√°logo.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clase _1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
