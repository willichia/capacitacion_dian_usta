{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d255c28-5ee0-4267-a41f-8c036039c114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Taler de Arquitecturas de Datos\n",
    "** Paso 1: Creando Nuestro Universo de Datos**\n",
    "\n",
    "隆Bienvenidos al taller pr谩ctico! Antes de poder construir y comparar arquitecturas, necesitamos la materia prima: los datos.\n",
    "\n",
    "En esta celda, ejecutaremos un script de PySpark que simula un ecosistema de datos completo para una empresa de E-commerce. Este no es un dataset est谩tico; es un generador que crear谩 un universo de datos controlado y realista para nuestro laboratorio.\n",
    "\n",
    "**驴Qu茅 Genera este Script?**\n",
    "Este c贸digo crear谩 varios \"activos de datos\" que simulan las diferentes fuentes que encontrar铆as en un entorno real:\n",
    "\n",
    "* **Tablas Estructuradas:** usuarios, productos y pedidos, simulando los datos de una base de datos transaccional.\n",
    "* **Logs Semi-Estructurados:** logs_web con la actividad de navegaci贸n de los usuarios.\n",
    "* **Archivos No Estructurados:** Facturas en formato de texto guardadas en DBFS, simulando la ingesta de archivos como PDFs.\n",
    "* **Documentos Complejos:** Un perfil_360 de cliente, simulando c贸mo se ver铆an los datos en una base de datos NoSQL como Cosmos DB o MongoDB.\n",
    "\n",
    "**Puntos Clave:**\n",
    "* **Librer铆a Faker:** Usamos esta librer铆a para generar datos que parecen reales (nombres, emails, fechas, etc.).\n",
    "* **Semilla de Reproducibilidad:** Hemos fijado una SEMILLA para que cada vez que se ejecute el script, genere exactamente los mismos datos. Elemental para que todos obtengamos los mismos resultados en los ejercicios.\n",
    "\n",
    "**Acci贸n:** Ejecuta esta celda para generar todos los DataFrames y archivos necesarios. 隆Este es el punto de partida para nuestro viaje a trav茅s de las arquitecturas de datos!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3a4adb-d601-4d98-91d5-68dd1894eaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script para generar datos simulados para el Taller de Arquitecturas de Datos\n",
    "# Este script utiliza PySpark y la librer铆a Faker para crear datos realistas,\n",
    "# incluyendo la simulaci贸n de archivos de texto (facturas) y logs web.\n",
    "#\n",
    "# Instrucciones en Databricks:\n",
    "# 1. Aseg煤rate de que la librer铆a 'Faker' est茅 instalada en tu cluster.\n",
    "#    Puedes hacerlo a trav茅s de la UI del cluster en la pesta帽a \"Libraries\".\n",
    "#    - PyPI -> package: Faker\n",
    "# 2. Copia y pega este c贸digo en una celda de un notebook de Databricks.\n",
    "# 3. Ejecuta la celda.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, ArrayType, MapType\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Inicializar Faker para generar datos falsos\n",
    "fake = Faker('es_ES') # Usar localizaci贸n en espa帽ol para datos m谩s realistas\n",
    "\n",
    "# --- SEMILLA PARA REPRODUCIBILIDAD ---\n",
    "# Establecemos una semilla para que los datos generados sean siempre los mismos en cada ejecuci贸n.\n",
    "# Esto es crucial para que todos los estudiantes trabajen con el mismo dataset.\n",
    "SEED = 42\n",
    "Faker.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- 1. Inicializaci贸n de Spark Session ---\n",
    "# En un notebook de Databricks, la sesi贸n de Spark ya est谩 creada como 'spark'.\n",
    "print(\"Spark Session iniciada.\")\n",
    "\n",
    "# --- 2. Funciones de Generaci贸n de Datos Relacionales ---\n",
    "\n",
    "def generar_usuarios(n=100):\n",
    "    \"\"\"Genera una lista de diccionarios de usuarios.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_usuario': 1000 + i,\n",
    "            'nombre': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'fecha_registro': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'ciudad': fake.city()\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_productos(n=50):\n",
    "    \"\"\"Genera una lista de diccionarios de productos.\"\"\"\n",
    "    categorias = ['Electr贸nica', 'Hogar', 'Ropa', 'Libros', 'Deportes']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_producto': 2000 + i,\n",
    "            'nombre_producto': fake.word().capitalize() + \" \" + fake.word(),\n",
    "            'categoria': random.choice(categorias),\n",
    "            'precio_unitario': round(random.uniform(5.0, 250.0), 2)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_pedidos(usuarios, productos, n=500):\n",
    "    \"\"\"Genera una lista de diccionarios de pedidos, vinculando usuarios y productos.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        producto = random.choice(productos)\n",
    "        cantidad = random.randint(1, 5)\n",
    "        data.append({\n",
    "            'id_pedido': 3000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'id_producto': producto['id_producto'],\n",
    "            'cantidad': cantidad,\n",
    "            'monto': round(cantidad * producto['precio_unitario'], 2),\n",
    "            'fecha_pedido': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_logs_web(usuarios, n=2000):\n",
    "    \"\"\"Genera una lista de diccionarios de logs de visitas web m谩s realistas.\"\"\"\n",
    "    paginas = ['/inicio', '/producto/detalle', '/carrito', '/checkout', '/perfil']\n",
    "    metodos = ['GET', 'GET', 'GET', 'POST', 'GET']\n",
    "    status = [200, 200, 200, 200, 404, 500]\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        data.append({\n",
    "            'id_log': 4000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'pagina_visitada': random.choice(paginas),\n",
    "            'metodo_http': random.choice(metodos),\n",
    "            'codigo_estado': random.choice(status),\n",
    "            'timestamp': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "print(\"Funciones de generaci贸n de datos creadas.\")\n",
    "\n",
    "# --- 3. Creaci贸n de DataFrames de Spark ---\n",
    "\n",
    "usuarios_data = generar_usuarios(100)\n",
    "productos_data = generar_productos(50)\n",
    "pedidos_data = generar_pedidos(usuarios_data, productos_data, 500)\n",
    "logs_web_data = generar_logs_web(usuarios_data, 2000)\n",
    "\n",
    "usuarios_df = spark.createDataFrame(usuarios_data)\n",
    "productos_df = spark.createDataFrame(productos_data)\n",
    "pedidos_df = spark.createDataFrame(pedidos_data)\n",
    "logs_web_df = spark.createDataFrame(logs_web_data)\n",
    "\n",
    "print(\"\\n--- DataFrames Relacionales Creados ---\")\n",
    "usuarios_df.show(3)\n",
    "productos_df.show(3)\n",
    "pedidos_df.show(3)\n",
    "logs_web_df.show(3, truncate=False)\n",
    "\n",
    "# --- 4. Generaci贸n de Datos No Estructurados (Simulaci贸n de Archivos) ---\n",
    "\n",
    "def generar_y_guardar_facturas_texto(pedidos, usuarios, productos, ruta_dbfs):\n",
    "    \"\"\"\n",
    "    Simula la creaci贸n de archivos de facturas (como si fueran PDFs convertidos a texto).\n",
    "    Guarda cada factura como un archivo .txt en la ruta de DBFS especificada.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerando archivos de facturas en la ruta: {ruta_dbfs}\")\n",
    "    \n",
    "    # Crear un mapa de usuarios y productos para b煤squeda f谩cil\n",
    "    mapa_usuarios = {u['id_usuario']: u for u in usuarios}\n",
    "    mapa_productos = {p['id_producto']: p for p in productos}\n",
    "    \n",
    "    # Asegurarse de que el directorio existe\n",
    "    dbutils.fs.mkdirs(ruta_dbfs)\n",
    "    \n",
    "    # Tomar una muestra de 50 pedidos para generar facturas\n",
    "    for pedido in random.sample(pedidos, 50):\n",
    "        usuario = mapa_usuarios.get(pedido['id_usuario'])\n",
    "        producto = mapa_productos.get(pedido['id_producto'])\n",
    "        \n",
    "        if not usuario or not producto:\n",
    "            continue\n",
    "            \n",
    "        # Crear el contenido de la factura como un string\n",
    "        contenido_factura = f\"\"\"\n",
    "        ========================================\n",
    "        FACTURA ELECTRNICA\n",
    "        ========================================\n",
    "        \n",
    "        N煤mero de Factura: INV-{pedido['id_pedido']}\n",
    "        Fecha: {pedido['fecha_pedido'].strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \n",
    "        --- Cliente ---\n",
    "        ID Cliente: {usuario['id_usuario']}\n",
    "        Nombre: {usuario['nombre']}\n",
    "        Email: {usuario['email']}\n",
    "        Ciudad: {usuario['ciudad']}\n",
    "        \n",
    "        --- Detalles del Pedido ---\n",
    "        ID Pedido: {pedido['id_pedido']}\n",
    "        \n",
    "        Descripci贸n                 Cantidad      Precio Unit.      Total\n",
    "        -----------------------------------------------------------------\n",
    "        {producto['nombre_producto']:<28}{pedido['cantidad']:<14}${producto['precio_unitario']:<16.2f}${pedido['monto']:.2f}\n",
    "        \n",
    "        ========================================\n",
    "        TOTAL A PAGAR: ${pedido['monto']:.2f}\n",
    "        ========================================\n",
    "        \"\"\"\n",
    "        \n",
    "        # Guardar el string en un archivo en DBFS\n",
    "        nombre_archivo = f\"factura_{pedido['id_pedido']}.txt\"\n",
    "        dbutils.fs.put(f\"{ruta_dbfs}/{nombre_archivo}\", contenido_factura, overwrite=True)\n",
    "        \n",
    "    print(f\"Se generaron 50 archivos de factura de ejemplo en {ruta_dbfs}\")\n",
    "    print(\"Los estudiantes pueden usar Auto Loader o spark.read.text() para ingerir estos datos.\")\n",
    "\n",
    "# Ejecutar la funci贸n para generar los archivos de factura\n",
    "ruta_facturas = \"/tmp/facturas_raw\"\n",
    "generar_y_guardar_facturas_texto(pedidos_data, usuarios_data, productos_data, ruta_facturas)\n",
    "\n",
    "\n",
    "# --- 5. Funci贸n para Generar Documentos de Perfil 360 (Simulaci贸n NoSQL) ---\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def crear_perfil_360(usuarios_df, pedidos_df, logs_web_df):\n",
    "    \"\"\"\n",
    "    Combina los DataFrames para crear un perfil 360 de cada cliente (simulaci贸n de Cosmos DB/MongoDB).\n",
    "    \"\"\"\n",
    "    pedidos_agrupados = pedidos_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"id_pedido\", \"id_producto\", \"monto\", \"fecha_pedido\")).alias(\"pedidos\"))\n",
    "    logs_agrupados = logs_web_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"pagina_visitada\", \"timestamp\")).alias(\"actividad_web\"))\n",
    "    \n",
    "    perfil_360_df = usuarios_df.join(pedidos_agrupados, \"id_usuario\", \"left\").join(logs_agrupados, \"id_usuario\", \"left\")\n",
    "    return perfil_360_df\n",
    "\n",
    "print(\"\\n--- Generando Documentos de Perfil 360 (Simulaci贸n NoSQL) ---\")\n",
    "perfil_360_df = crear_perfil_360(usuarios_df, pedidos_df, logs_web_df)\n",
    "\n",
    "print(\"Mostrando una muestra de los perfiles 360:\")\n",
    "perfil_360_df.show(3, truncate=False)\n",
    "\n",
    "# --- 6. Visualizaci贸n de un Documento JSON ---\n",
    "\n",
    "print(\"\\n--- Ejemplo de un Documento JSON para el Perfil 360 ---\")\n",
    "primer_perfil_json = perfil_360_df.first().asDict(recursive=True)\n",
    "print(json.dumps(primer_perfil_json, indent=4, default=str))\n",
    "\n",
    "#\n",
    "# Fin del Script\n",
    "# Los DataFrames y los archivos de texto ya est谩n listos para el taller.\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clase _1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
